{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9bc461-9028-4553-90c4-949bd897ae8b",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Answer 1...\n",
    "\n",
    "Gradient Boosting Regression is a machine learning algorithm that is used for regression problems. It is a type of boosting algorithm that combines multiple weak regression models into a single strong regression model. The algorithm works by iteratively fitting weak models to the residual errors of the previous models, thus gradually reducing the error. The final model is a weighted sum of the weak models, where the weights are learned during the training process. Gradient Boosting Regression is a popular algorithm for solving regression problems in a variety of domains, such as finance, healthcare, and e-commerce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70603a7-0cd3-4360-980e-c4d054e6c231",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8725ba8-88e1-4a6f-8e02-1a8a4d3c8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.train_mean = None\n",
    "        self.train_std = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train_mean = np.mean(y)\n",
    "        self.train_std = np.std(y)\n",
    "        y = (y - self.train_mean) / self.train_std\n",
    "        \n",
    "        f = np.zeros(len(X))\n",
    "        for i in range(self.n_estimators):\n",
    "            r = y - f\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, r)\n",
    "            f += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        y_pred = (y_pred * self.train_std) + self.train_mean\n",
    "        return y_pred\n",
    "        \n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1)\n",
    "X_train, y_train = X[:80], y[:80]\n",
    "X_test, y_test = X[80:], y[80:]\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "print('Mean squared error: %.2f' % mse)\n",
    "print('R-squared: %.2f' % r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291939c-7746-494f-9506-340177356b98",
   "metadata": {},
   "source": [
    "In this implementation, we first define the GradientBoostingRegressor class, which takes three hyperparameters as inputs: n_estimators (number of trees), learning_rate (step size shrinkage), and max_depth (maximum depth of the decision trees). We also define an internal list self.trees to store the decision trees, and two variables self.train_mean and self.train_std to store the mean and standard deviation of the training labels.\n",
    "\n",
    "In the fit method, we first normalize the training labels using z-score normalization. We then initialize the predictions f to zero and loop over the number of trees. At each iteration, we compute the residual errors r between the training labels and the current predictions f, and fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8e972-8aec-4964-a9f8-ab78d25de436",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34a784-e496-4851-b6f2-2489799e9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best hyperparameters:', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab97b9b-152b-4f98-80bc-d18d50253767",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Answer 4...\n",
    "\n",
    "In Gradient Boosting, a weak learner is a simple and relatively weak model that can be trained quickly and that makes predictions that are slightly better than random. The most common type of weak learner used in Gradient Boosting is a decision tree with a small maximum depth. The idea behind using weak learners is to combine many of them to create a strong ensemble model that can make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea195ee3-f2e4-40ea-8dbb-b3fa2b331e95",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Answer 5...\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to combine many weak learners to create a strong ensemble model that can make accurate predictions. The algorithm works by iteratively adding new weak learners to the ensemble and adjusting the predictions of the previous learners to minimize the error on the training data. At each iteration, the algorithm fits a new weak learner to the residual errors of the previous learners, and adds its predictions to the overall ensemble. The weight given to each weak learner in the ensemble is determined by its performance on the training data. By combining many weak learners, Gradient Boosting can create a complex model that can capture the non-linear relationships between the input features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d2a40-6383-4916-8a71-6ce24e1dbb17",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Answer 6...\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in the following way:\n",
    "\n",
    "Initialize the predictions to the mean of the target variable.\n",
    "For each iteration:\n",
    "\n",
    "a. Compute the residuals between the predictions and the actual target values.\n",
    "\n",
    "b. Train a weak learner on the residuals.\n",
    "\n",
    "c. Update the predictions by adding the predictions of the weak learner multiplied by a small learning rate to the previous predictions.\n",
    "\n",
    "d. Repeat steps a-c until the desired number of weak learners has been trained.\n",
    "The final predictions are the sum of the predictions of all the weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877c118-d26e-4e35-9462-c821a3e0b3f0",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "\n",
    "Answer 7...\n",
    "\n",
    "\n",
    "The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm are as follows:\n",
    "\n",
    "1) Define the loss function that you want to optimize. This could be, for example, mean squared error or mean absolute error.\n",
    "\n",
    "2) Initialize the predictions to the mean of the target variable.\n",
    "\n",
    "3) For each iteration:\n",
    "\n",
    "a. Compute the negative gradient of the loss function with respect to the current predictions\n",
    "\n",
    "b. Train a weak learner on the negative gradient values as the target variable and the input features as the predictors.\n",
    "\n",
    "c. Compute the predictions of the weak learner and multiply them by a small learning rate.\n",
    "\n",
    "d. Add the predictions of the weak learner to the previous predictions.\n",
    "\n",
    "e. Repeat steps a-d until the desired number of weak learners has been trained.\n",
    "\n",
    "4) The final predictions are the sum of the predictions of all the weak learners.\n",
    "\n",
    "5) To make a prediction on a new data point, apply the weak learners sequentially to the input features and sum their predictions.\n",
    "\n",
    "In summary, the Gradient Boosting algorithm builds an ensemble of weak learners by iteratively fitting them to the negative gradient of the loss function with respect to the current predictions. The final predictions are the sum of the predictions of all the weak learners, weighted by a learning rate. By minimizing the loss function, the algorithm can learn a model that can make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8041ca-aa28-470d-a8e0-453169b6cc26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
