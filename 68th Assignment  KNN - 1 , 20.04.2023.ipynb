{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c766bc7-582c-4bfd-bbd5-1cb5b61ec1d9",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ea5f3-8759-4012-993c-187d93ac52cb",
   "metadata": {},
   "source": [
    "A1.\n",
    "\n",
    "The KNN algorithm is a type of machine learning algorithm that can be used for both classification and regression tasks. The algorithm works by finding the K nearest data points to a given query point, based on some distance metric, and using the labels or values of those points to make a prediction for the query point.\n",
    "\n",
    "A2.\n",
    "\n",
    "The value of K in KNN can be chosen by using a technique called cross-validation, where the data is split into training and validation sets, and the algorithm is trained and evaluated for different values of K. The value of K that gives the best performance on the validation set is then chosen as the final value.\n",
    "\n",
    "A3.\n",
    "\n",
    "The KNN classifier is used for classification tasks, where the goal is to predict a discrete label or class for a given input. The KNN regressor, on the other hand, is used for regression tasks, where the goal is to predict a continuous value for a given input.\n",
    "\n",
    "A4.\n",
    "\n",
    "The performance of KNN can be measured using metrics such as accuracy, precision, recall, F1 score, and mean squared error, depending on the task at hand. These metrics can be computed using cross-validation, where the data is split into training and validation sets, and the algorithm is evaluated on the validation set.\n",
    "\n",
    "A5.\n",
    "\n",
    "The curse of dimensionality in KNN refers to the fact that as the number of features or dimensions in the data increases, the distance between any two points in the data becomes less meaningful, making it harder to find meaningful nearest neighbors. This can be addressed by using feature selection or dimensionality reduction techniques, or by using more sophisticated distance metrics that take into account the structure of the data.\n",
    "\n",
    "A6.\n",
    "\n",
    "Missing values in KNN can be handled by imputing them with some value, such as the mean or median of the other values in the same feature, or by using more sophisticated imputation techniques such as k-nearest neighbor imputation.\n",
    "\n",
    "A7.\n",
    "\n",
    "The performance of the KNN classifier and regressor depends on the nature of the problem at hand. The KNN classifier is better suited for problems where the output is discrete, such as image classification or spam filtering, while the KNN regressor is better suited for problems where the output is continuous, such as predicting the price of a house based on its features.\n",
    "\n",
    "A8.\n",
    "\n",
    "The strengths of the KNN algorithm include its simplicity, interpretability, and ability to handle non-linear relationships between features and the output. Its weaknesses include its sensitivity to the choice of distance metric and the value of K, its computational complexity, and its inability to handle high-dimensional data. These weaknesses can be addressed by using more sophisticated distance metrics, choosing an appropriate value of K, using dimensionality reduction techniques, or using more efficient algorithms for nearest neighbor search.\n",
    "\n",
    "A9.\n",
    "\n",
    "Euclidean distance in KNN is a measure of the straight-line distance between two points in the feature space, while Manhattan distance is a measure of the distance between two points that takes into account only the horizontal and vertical distances between them, ignoring any diagonal distances.\n",
    "\n",
    "A10.\n",
    "\n",
    "Feature scaling in KNN is important because the distance metric used by the algorithm is sensitive to differences in scale between the features. If the features are not scaled properly, then some features may dominate the distance metric and others may be ignored. Feature scaling can be done by normalizing the features to have zero mean and unit variance, or by scaling them to a fixed range, such as [0,1].\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f65b5-166d-429d-b468-52a6ec46c218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
