{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41b9530-af07-4dbb-bc4d-2ddd5c9b1c1d",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "Q2. What are the key challenges in anomaly detection?\n",
    "\n",
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n",
    "\n",
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1b94e-7dd4-4d9b-83b4-237fe36c4c9f",
   "metadata": {},
   "source": [
    "Answer 1...\n",
    "\n",
    "Anomaly detection is a technique used to identify patterns or instances in data that deviate significantly from the norm or expected behavior. Its purpose is to identify unusual or rare events, outliers, or anomalies that do not conform to the general patterns in the data. Anomalies can be indicative of potential fraud, errors, cybersecurity threats, or other unusual events that require further investigation.\n",
    "\n",
    "Answer 2...\n",
    "\n",
    "Some key challenges in anomaly detection include:\n",
    "\n",
    "a) Lack of labeled data: Anomaly detection often deals with unlabeled data, which makes it challenging to define normal behavior accurately.\n",
    "\n",
    "b) Imbalanced data: Anomalies are typically rare events compared to normal instances, leading to imbalanced datasets, which can affect the performance of anomaly detection algorithms.\n",
    "\n",
    "c) Dynamic and evolving anomalies: Anomalies can change over time or adapt to new patterns, requiring detection methods to be flexible and adaptable.\n",
    "\n",
    "d) Feature selection and dimensionality: Identifying relevant features or reducing high-dimensional data while preserving anomalous patterns can be a challenging task.\n",
    "\n",
    "e) Interpretability: Understanding the reasons behind the detected anomalies and providing actionable insights to stakeholders can be difficult.\n",
    "\n",
    "Answer 3...\n",
    "\n",
    "Unsupervised anomaly detection and supervised anomaly detection differ in the availability of labeled data during the training phase:\n",
    "\n",
    "a) Unsupervised anomaly detection: In this approach, only normal data instances are available during training, and the goal is to identify unusual patterns or outliers without prior knowledge of anomalies. It aims to learn the underlying distribution of normal data and flag instances that deviate significantly from this learned distribution.\n",
    "\n",
    "b) Supervised anomaly detection: Here, both normal and anomalous instances are available during training, and the algorithm learns the characteristics of both classes. The goal is to classify new instances as either normal or anomalous based on the learned model. Supervised anomaly detection requires labeled training data, which can be a limitation in many real-world scenarios.\n",
    "\n",
    "Answer 4...\n",
    "\n",
    "The main categories of anomaly detection algorithms include:\n",
    "\n",
    "a) Statistical methods: These algorithms assume that the normal data instances can be described by a statistical distribution (e.g., Gaussian distribution), and anomalies are identified as data points that have a low probability of being generated by the assumed distribution.\n",
    "\n",
    "b) Machine learning-based methods: These algorithms utilize various machine learning techniques, such as clustering, classification, or density estimation, to identify anomalies. They often learn the patterns of normal behavior and detect deviations from it.\n",
    "\n",
    "c) Proximity-based methods: These methods measure the distance or similarity between data points and identify anomalies as instances that are significantly different or distant from the majority of the data.\n",
    "\n",
    "d) Information-theoretic methods: These algorithms use measures of entropy or information gain to detect anomalies by identifying data points that have high information content or unexpectedness compared to the rest of the data.\n",
    "\n",
    "Answer 5...\n",
    "\n",
    "Distance-based anomaly detection methods make certain assumptions:\n",
    "\n",
    "a) Normal instances are grouped together and form dense neighborhoods.\n",
    "\n",
    "b) Anomalies are located far away from normal instances and have fewer neighboring points.\n",
    "\n",
    "c) The distance metric used is appropriate for capturing the differences between data points.\n",
    "\n",
    "These assumptions imply that anomalies will have a larger distance to their nearest neighbors compared to normal instances.\n",
    "\n",
    "Answer 6...\n",
    "\n",
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores as follows:\n",
    "\n",
    "a) For each data point, it calculates the local density by measuring the average distance to its k nearest neighbors.\n",
    "\n",
    "b) It compares the local density of a data point with the local densities of its k nearest neighbors.\n",
    "\n",
    "c) If the local density of a data point is significantly lower than the densities of its neighbors, it indicates that the data point is in a sparser region and likely an anomaly.\n",
    "\n",
    "d) The anomaly score is calculated as the ratio of the average local density of the data point's k nearest neighbors to its own local density. A higher score indicates a higher likelihood of being an anomaly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c95010-93ab-48ed-adee-297e0128daf6",
   "metadata": {},
   "source": [
    "Answwer 7...\n",
    "\n",
    "The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "a) n_estimators: It represents the number of isolation trees to be created. A higher number of trees can improve the performance but also increase the computational cost.\n",
    "\n",
    "b) max_samples: It determines the number of samples drawn from the dataset to build each isolation tree. It can be an absolute number or a fraction of the total number of data points.\n",
    "\n",
    "c) contamination: It specifies the expected proportion of anomalies in the dataset. It helps in determining the threshold for classifying instances as anomalies.\n",
    "\n",
    "d) max_features: It controls the number of features to consider when splitting a node in the isolation tree. It can be an absolute number or a fraction of the total number of features.\n",
    "\n",
    "Answer 8...\n",
    "\n",
    "The anomaly score using KNN (K-nearest neighbors) with K=10 for a data point that has only 2 neighbors of the same class within a radius of 0.5 cannot be determined solely based on this information. The anomaly score in KNN is typically computed based on the distance to the Kth nearest neighbor. However, without knowing the distances to the other neighbors or the distribution of distances in the dataset, it is not possible to determine the anomaly score accurately.\n",
    "\n",
    "Answer 9...\n",
    "\n",
    "In the Isolation Forest algorithm, the average path length is used to compute the anomaly score. A data point with an average path length of 5.0 compared to the average path length of the trees in the Isolation Forest algorithm suggests that it takes fewer steps to isolate the data point. A lower average path length indicates that the data point is more likely to be an anomaly. However, the specific calculation of the anomaly score depends on the implementation and threshold settings of the Isolation Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc13780-8141-4195-81aa-88747c55ba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
