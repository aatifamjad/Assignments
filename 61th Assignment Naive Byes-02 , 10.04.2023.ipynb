{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65faee39-6731-4d5a-9710-be0d554402f8",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a5ff5-312f-4745-a365-b0b1a0286533",
   "metadata": {},
   "source": [
    "Answer 1...\n",
    "\n",
    "To calculate the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Let S be the event that an employee is a smoker, and H be the event that an employee uses the health insurance plan. Then, we want to calculate P(S|H), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Bayes' theorem tells us that:\n",
    "\n",
    "P(S|H) = P(H|S) * P(S) / P(H)\n",
    "\n",
    "We are given that 70% of the employees use the health insurance plan, so P(H) = 0.7. We are also given that 40% of the employees who use the plan are smokers, so P(S âˆ© H) = P(H|S) * P(S) = 0.4 * P(S). Therefore, we have:\n",
    "\n",
    "P(S|H) = 0.4 * P(S) / 0.7\n",
    "\n",
    "To solve for P(S), we need more information. Without knowing the overall proportion of smokers in the employee population, we cannot calculate P(S) directly. However, we can use the fact that P(S) + P(~S) = 1, where ~S is the event that an employee is not a smoker. Let x be the proportion of employees who are smokers, so P(S) = x and P(~S) = 1 - x. Then, we have:\n",
    "\n",
    "0.4 * x / 0.7 = P(S|H) = P(H|S) * P(S) / P(H) = 0.4 * x / (0.7 * x + 0.3 * (1 - x))\n",
    "\n",
    "Solving for x, we get:\n",
    "\n",
    "x = 0.5714\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is:\n",
    "\n",
    "P(S|H) = 0.4 * 0.5714 / 0.7 = 0.327\n",
    "\n",
    "So the probability that an employee who uses the health insurance plan is a smoker is 0.327.\n",
    "\n",
    "Answer 2...\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is in the way they model the data. Bernoulli Naive Bayes assumes that the input features are binary (e.g., a word is either present or absent in a document), while Multinomial Naive Bayes assumes that the input features are counts (e.g., the frequency of a word in a document).\n",
    "\n",
    "Answer 3...\n",
    "\n",
    "Bernoulli Naive Bayes can handle missing values by assuming that a missing value is equivalent to the feature being absent. So, if a document has a missing feature, it is assumed that the feature is not present in the document.\n",
    "\n",
    "Answer 4...\n",
    "\n",
    "Gaussian Naive Bayes can be used for multi-class classification by training multiple binary classifiers, one for each class. For example, if there are three classes, A, B, and C, three binary classifiers can be trained: one to distinguish A from non-A, one to distinguish B from non-B, and one to distinguish C from non-C. To make a prediction for a new instance, the classifier with the highest posterior probability can be selected.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6950a4d-2f02-4ecd-9700-535866fa7962",
   "metadata": {},
   "source": [
    "Answer 5...\n",
    "\n",
    "For this assignment, we will be using the Spambase dataset from the UCI Machine Learning Repository to implement and compare the performance of three variants of Naive Bayes classifiers.\n",
    "\n",
    "First, we need to download and prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb524ff1-695f-43d3-ada6-659d2a5afaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None)\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9536e4-01ca-4213-9cc5-7dcd730d0854",
   "metadata": {},
   "source": [
    "We split the dataset into train and test sets, with 20% of the data used for testing.\n",
    "\n",
    "Next, we will implement the three variants of Naive Bayes classifiers and evaluate their performance using 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f761fb43-21b5-4a45-a18a-79b15a6a2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Accuracy: 0.89 (+/- 0.04)\n",
      "Multinomial Naive Bayes Accuracy: 0.79 (+/- 0.03)\n",
      "Gaussian Naive Bayes Accuracy: 0.82 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "bnb_scores = cross_val_score(bnb, X_train, y_train, cv=10)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb_scores = cross_val_score(mnb, X_train, y_train, cv=10)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb, X_train, y_train, cv=10)\n",
    "\n",
    "# print mean and standard deviation of scores\n",
    "print(\"Bernoulli Naive Bayes Accuracy: %0.2f (+/- %0.2f)\" % (bnb_scores.mean(), bnb_scores.std() * 2))\n",
    "print(\"Multinomial Naive Bayes Accuracy: %0.2f (+/- %0.2f)\" % (mnb_scores.mean(), mnb_scores.std() * 2))\n",
    "print(\"Gaussian Naive Bayes Accuracy: %0.2f (+/- %0.2f)\" % (gnb_scores.mean(), gnb_scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3399dd1-7532-41aa-bf79-60fda4181828",
   "metadata": {},
   "source": [
    "The above code prints the mean and standard deviation of accuracy scores for each classifier. We can calculate additional performance metrics using scikit-learn's classification_report function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c109921-59d1-47b6-ad1f-b94a4f41145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       531\n",
      "           1       0.91      0.80      0.85       390\n",
      "\n",
      "    accuracy                           0.88       921\n",
      "   macro avg       0.89      0.87      0.88       921\n",
      "weighted avg       0.88      0.88      0.88       921\n",
      "\n",
      "Multinomial Naive Bayes Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       531\n",
      "           1       0.76      0.72      0.74       390\n",
      "\n",
      "    accuracy                           0.79       921\n",
      "   macro avg       0.78      0.78      0.78       921\n",
      "weighted avg       0.79      0.79      0.79       921\n",
      "\n",
      "Gaussian Naive Bayes Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.73      0.82       531\n",
      "           1       0.72      0.95      0.82       390\n",
      "\n",
      "    accuracy                           0.82       921\n",
      "   macro avg       0.83      0.84      0.82       921\n",
      "weighted avg       0.85      0.82      0.82       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bnb.fit(X_train, y_train)\n",
    "bnb_preds = bnb.predict(X_test)\n",
    "print(\"Bernoulli Naive Bayes Performance:\")\n",
    "print(classification_report(y_test, bnb_preds))\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb.fit(X_train, y_train)\n",
    "mnb_preds = mnb.predict(X_test)\n",
    "print(\"Multinomial Naive Bayes Performance:\")\n",
    "print(classification_report(y_test, mnb_preds))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_preds = gnb.predict(X_test)\n",
    "print(\"Gaussian Naive Bayes Performance:\")\n",
    "print(classification_report(y_test, gnb_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066d7f4-eae1-42c8-9835-69434573598b",
   "metadata": {},
   "source": [
    "The above code prints the precision, recall, f1-score, and support for each class for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75f1dd-b553-483c-8536-ad9132da6bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
