{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664f9f29-d641-4b0f-be6a-d71368b06e2b",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726ac55-ffeb-4215-96f3-b419cc76a9e4",
   "metadata": {},
   "source": [
    "Answer 1...\n",
    "\n",
    "The mathematical formula for a linear SVM is:\n",
    "\n",
    "$y(x) = w^T x + b$\n",
    "\n",
    "where $x$ is the input data, $w$ is the weight vector, $b$ is the bias term, and $y(x)$ is the predicted output.\n",
    "\n",
    "Answer 2...\n",
    "\n",
    "The objective function of a linear SVM is to maximize the margin between the decision boundary and the closest points of each class. The objective function is formulated as:\n",
    "\n",
    "$\\min_{w,b} \\frac{1}{2} ||w||^2$\n",
    "\n",
    "subject to $y_i (w^T x_i + b) \\geq 1$ for all $i$\n",
    "\n",
    "where $y_i$ is the class label of the $i$-th training example, $x_i$ is the input data, and $||w||$ is the norm of the weight vector.\n",
    "\n",
    "Answer 3...\n",
    "\n",
    "The kernel trick in SVM is a technique used to transform the input data into a higher-dimensional space, where the data may be more separable. The kernel function takes two input vectors and returns a scalar that represents the similarity between them. The kernel function is used to compute the inner product between two vectors in the high-dimensional space, without explicitly computing the transformation.\n",
    "\n",
    "Answer 4...\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary of the SVM. These points play a crucial role in defining the decision boundary and maximizing the margin between the classes. The support vectors are the points that satisfy the constraint $y_i (w^T x_i + b) = 1$. Any other point that satisfies this constraint is also a support vector. The importance of support vectors lies in the fact that changing their position or removing them can drastically alter the position of the decision boundary.\n",
    "\n",
    "For example, in a binary classification problem where the data points are linearly separable, the support vectors would be the points that lie on the margin or on the wrong side of the margin.\n",
    "\n",
    "Answer 5...\n",
    "\n",
    "Hyperplane: In SVM, the decision boundary is a hyperplane that separates the data points into different classes. In a two-dimensional space, a hyperplane is a line that separates the data points.\n",
    "\n",
    "Marginal plane: The marginal plane is the plane that is parallel to the hyperplane and passes through the closest points of each class. The distance between the hyperplane and the marginal plane is the margin.\n",
    "\n",
    "Soft margin: In a soft margin SVM, the margin is allowed to be violated for some data points, in order to achieve a better fit to the data. The soft margin SVM introduces a slack variable that allows the margin to be violated by a certain amount.\n",
    "\n",
    "Hard margin: In a hard margin SVM, the margin is not allowed to be violated. This means that the SVM will only find a solution if the data is linearly separable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff1a05-3ade-4f52-b1af-b73cfa91f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6...\n",
    "\n",
    "# Here is an implementation of a linear SVM classifier using the Iris dataset from scikit-learn:\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d82bdf-347b-4220-90f9-85a5736fb658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
