{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508b566a-aafd-4393-94a6-25467a68f2c8",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895a1a3-82b3-47ef-a188-c6b6bc61e6e2",
   "metadata": {},
   "source": [
    "Answer 1..\n",
    "\n",
    "The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is the way they measure distance between two data points. The Euclidean distance is the straight-line distance between two points in a Cartesian plane, while the Manhattan distance is the sum of the absolute differences of their coordinates. The choice of distance metric can affect the performance of the KNN classifier or regressor, as it determines which data points are considered neighbors and therefore influence the prediction. In general, the Euclidean distance tends to work better when the data is continuous and the features are highly correlated, while the Manhattan distance is more suitable for sparse and high-dimensional data.\n",
    "\n",
    "Answer 2...\n",
    "\n",
    "The optimal value of k for a KNN classifier or regressor depends on the specific problem and the characteristics of the data. One common approach is to use cross-validation to evaluate the performance of different k values and choose the one that yields the best results. Another technique is to use a grid search to test a range of k values and select the one with the highest accuracy or lowest error.\n",
    "\n",
    "Answer 3...\n",
    "\n",
    "The choice of distance metric can have a significant impact on the performance of a KNN classifier or regressor. Different distance metrics may be more suitable for different types of data, and it is important to choose the right one based on the specific problem and the characteristics of the data. For example, the cosine distance is often used for text data, while the Hamming distance is used for binary data. In general, the choice of distance metric should be based on the underlying structure of the data and the problem at hand.\n",
    "\n",
    "Answer 4...\n",
    "\n",
    "Some common hyperparameters in KNN classifiers and regressors include the number of neighbors (k), the distance metric, and the weighting scheme (uniform or distance-based). These hyperparameters can affect the performance of the model, and tuning them can improve its accuracy or reduce its error. One approach to tuning hyperparameters is to use a grid search to test different combinations of values and select the ones that yield the best results.\n",
    "\n",
    "Answer 5...\n",
    "\n",
    "The size of the training set can affect the performance of a KNN classifier or regressor, as a smaller training set may not capture the full range of variation in the data and lead to overfitting, while a larger training set may increase the computational complexity and reduce the accuracy of the model. Techniques such as cross-validation can be used to optimize the size of the training set by evaluating the performance of the model on different subsets of the data.\n",
    "\n",
    "Answer 6...\n",
    "\n",
    "Some potential drawbacks of using KNN as a classifier or regressor include its sensitivity to noise, its tendency to be affected by irrelevant features, and its high computational complexity for large datasets. To overcome these drawbacks, one can use feature selection techniques to identify the most relevant features, apply pre-processing steps such as normalization or scaling to reduce the impact of noise, or use dimensionality reduction methods such as PCA to reduce the computational complexity. Additionally, using a weighted distance metric or applying kernel methods can also improve the performance of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce1f38-3023-4ad9-8a46-9ef3b28e1329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
